{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import scipy.io as spio\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_vgg = models.vgg16(pretrained=True)\n",
    "\n",
    "print(model_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN16(nn.Module):\n",
    "    def __init__(self,model,hidden =64,kernel_size=3,padding = 1, num_classes = 21):\n",
    "        super(FCN16,self).__init__()\n",
    "        \n",
    "        self.block1 = nn.Sequential(*list(model.features)[:5])\n",
    "        self.block2 = nn.Sequential(*list(model.features)[5:10])\n",
    "        self.block3 = nn.Sequential(*list(model.features)[10:17])\n",
    "        self.block4 = nn.Sequential(*list(model.features)[17:24])\n",
    "        self.block5 = nn.Sequential(*list(model.features)[24:31])\n",
    "        \n",
    "        model.classifier[0] = nn.Conv2d(512,4096,7)\n",
    "        model.classifier[3] = nn.Conv2d(4096,4096,1)\n",
    "        model.classifier[6] = nn.Conv2d(4096,num_classes,1)\n",
    "        \n",
    "        self.fc6 = nn.Sequential(*list(model.classifier)[0:3])\n",
    "        self.fc7 = nn.Sequential(*list(model.classifier)[3:6])\n",
    "        self.block_score = model.classifier[6]\n",
    "        \n",
    "        self.score_pool4 = nn.Conv2d(8*hidden,num_classes, kernel_size=1)\n",
    "        \n",
    "        self.upscore2 = nn.ConvTranspose2d(num_classes,num_classes,4,stride=2,bias=False)\n",
    "        self.upscore16 = nn.ConvTranspose2d(num_classes,num_classes,32,stride = 16, bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        pred1 = self.block1(x)\n",
    "        pred2 = self.block2(pred1)\n",
    "        pred3 = self.block3(pred2)\n",
    "        pred4 = self.block4(pred3)\n",
    "        pred5 = self.block5(pred4)\n",
    "        \n",
    "        pred6 = self.fc6(pred5)\n",
    "        pred7 = self.fc7(pred6)\n",
    "        score = self.block_score(pred7)\n",
    "        upscore2 = self.upscore2(score)\n",
    "        \n",
    "        pred4_1 = self.score_pool4(pred4)\n",
    "        \n",
    "        upscore16 = self.upscore16(upscore2+pred4_1)\n",
    "        return upscore16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "fcn_model = FCN16(model)\n",
    "\n",
    "print(fcn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterCrop():\n",
    "    \"\"\"\n",
    "    Crops the center of the image and its dense labels in a sample.\n",
    "    Note that PIL Image instances are casted to numpy ndarrays in this step.\n",
    "    \n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        img, target = sample\n",
    "        # convert PIL Image to numpy.ndarray\n",
    "        img = np.array(img)\n",
    "        target = np.array(target)\n",
    "        new_h, new_w = self.output_size\n",
    "        \n",
    "        # zero-pad if the width or height is less than the output_size\n",
    "        if img.shape[0] < new_h:\n",
    "            # zero-pad vertically\n",
    "            pad_width = new_h - img.shape[0]\n",
    "            up_pad = pad_width // 2\n",
    "            bottom_pad = pad_width - up_pad\n",
    "            img = np.pad(img, ((up_pad, bottom_pad), (0, 0), (0, 0)), 'constant', constant_values=(0, 0))\n",
    "            target = np.pad(target, ((up_pad, bottom_pad), (0, 0)), 'constant', constant_values=(0, 0))\n",
    "            \n",
    "        if img.shape[1] < new_w:\n",
    "            # zero-pad horizontally\n",
    "            pad_width = new_w - img.shape[1]\n",
    "            left_pad = pad_width // 2\n",
    "            right_pad = pad_width - left_pad\n",
    "            img = np.pad(img, ((0, 0), (left_pad, right_pad), (0, 0)), 'constant', constant_values=(0, 0))\n",
    "            target = np.pad(target, ((0, 0), (left_pad, right_pad)), 'constant', constant_values=(0, 0))\n",
    "\n",
    "            \n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        top = (h - new_h) // 2\n",
    "        left = (w - new_w) // 2\n",
    "        \n",
    "        img = img[top: top + new_h, left: left + new_w]\n",
    "        target = target[top: top + new_h, left: left + new_w]\n",
    "        assert img.shape[:2] == target.shape[:2]\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_dense_label(model, base_path, img_name, center_crop=(224, 224)):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 20))\n",
    "    voc_colors = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n",
    "                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n",
    "                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n",
    "                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n",
    "                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n",
    "                [0, 64, 128]]\n",
    "\n",
    "    voc_classes = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "                   'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
    "                   'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "                   'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']\n",
    "    \n",
    "    img_path = os.path.join(base_path + 'img', img_name + '.jpg')\n",
    "    gt_path = os.path.join(base_path + 'cls', img_name + '.mat')\n",
    "    ind2color = {}\n",
    "    legend_elements = []\n",
    "    for ind, color in enumerate(voc_colors):\n",
    "        ind2color[ind] = (color, voc_classes[ind])\n",
    "        legend_elements.append(Line2D([0], [0], color=np.array(color)/255, lw=6))\n",
    "    \n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = np.array(img)\n",
    "    \n",
    "    gt_mat = spio.loadmat(gt_path, mat_dtype=True, squeeze_me=True, struct_as_record=False)\n",
    "    gt_mat = gt_mat['GTcls'].Segmentation\n",
    "    \n",
    "    \n",
    "    if center_crop:\n",
    "        crop = CenterCrop(center_crop)\n",
    "        img, gt_mat = crop((img, gt_mat))\n",
    "        \n",
    "    ax1.set_title('original image')\n",
    "    ax1.imshow(img)\n",
    "    \n",
    "    img = np.transpose(img, [2, 0, 1])\n",
    "    img = torch.from_numpy(img).to(torch.float32)\n",
    "    img = transforms.functional.normalize(img, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    img = torch.unsqueeze(img, dim=0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "    \n",
    "    pred = torch.squeeze(pred, dim=0)\n",
    "    \n",
    "    \n",
    "    pred = pred.numpy()\n",
    "    height, width = pred.shape\n",
    "    colored_pred = np.zeros((height, width, 3), dtype=np.int32)\n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "            colored_pred[h, w] = ind2color[pred[h, w]][0]\n",
    "    \n",
    "    ax2.set_title('predicted feature map')\n",
    "    ax2.imshow(colored_pred)\n",
    "    \n",
    "    height, width = gt_mat.shape\n",
    "    colored_gt_mat = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "            colored_gt_mat[h, w] = ind2color[gt_mat[h, w]][0]\n",
    "            \n",
    "    ax3.set_title('ground true dense label')\n",
    "    ax3.legend(handles=legend_elements, labels=voc_classes, loc='upper center', bbox_to_anchor=(1.5, 1.2))\n",
    "    ax3.imshow(colored_gt_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_inference(model, base_path, img_name, center_crop=(512, 512)):\n",
    "    \"\"\"\n",
    "    Plot the original image, predicted dense labels and the ground-true labels.\n",
    "    Parameters:\n",
    "    - model: PyTorch model\n",
    "    - base_path: path to the augmented Pascal VOC dataset\n",
    "    - img_name: image file name without format extension\n",
    "                e.g. 2008_000073 is the img_path for 2008_000073.jpg and 2008_000073.mat\n",
    "    - center_crop: whether crop to certain height, width for better visualization\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 20))\n",
    "    voc_colors = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n",
    "                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n",
    "                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n",
    "                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n",
    "                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n",
    "                [0, 64, 128]]\n",
    "\n",
    "    voc_classes = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "                   'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
    "                   'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "                   'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']\n",
    "    \n",
    "    img_path = os.path.join(base_path + 'img', img_name + '.jpg')\n",
    "    gt_path = os.path.join(base_path + 'cls', img_name + '.mat')\n",
    "    ind2color = {}\n",
    "    legend_elements = []\n",
    "    for ind, color in enumerate(voc_colors):\n",
    "        ind2color[ind] = (color, voc_classes[ind])\n",
    "        legend_elements.append(Line2D([0], [0], color=np.array(color)/255, lw=6))\n",
    "        \n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = np.array(img)\n",
    "    gt_mat = spio.loadmat(gt_path, mat_dtype=True, squeeze_me=True, struct_as_record=False)\n",
    "    gt_mat = gt_mat['GTcls'].Segmentation\n",
    "    \n",
    "    \n",
    "    if center_crop:\n",
    "        crop = CenterCrop(center_crop)\n",
    "        img, gt_mat = crop((img, gt_mat))\n",
    "    \n",
    "    ax1.set_title('original image')\n",
    "    ax1.imshow(img)\n",
    "    \n",
    "    img = np.transpose(img, [2, 0, 1])\n",
    "    img = torch.from_numpy(img).to(torch.float32)\n",
    "    img = torch.div(img, 255)\n",
    "    img = transforms.functional.normalize(img, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    # make img a 4-dimension tensor NCHW, N == 1\n",
    "    img = torch.unsqueeze(img, dim=0)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "    \n",
    "    # squeeze the dense label back to a 3-dimension tensor, CHW\n",
    "    pred = torch.squeeze(pred, dim=0)\n",
    "    \n",
    "    pred = pred.numpy()\n",
    "    height, width = pred.shape\n",
    "    colored_pred = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "            colored_pred[h, w] = ind2color[pred[h, w]][0]\n",
    "    \n",
    "    ax2.set_title('predicted feature map')\n",
    "    ax2.imshow(colored_pred)\n",
    "    \n",
    "    height, width = gt_mat.shape\n",
    "    colored_gt_mat = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "            colored_gt_mat[h, w] = ind2color[gt_mat[h, w]][0]\n",
    "            \n",
    "    ax3.set_title('ground true dense label')\n",
    "    ax3.legend(handles=legend_elements, labels=voc_classes, loc='upper center', bbox_to_anchor=(1.5, 1.2))\n",
    "    ax3.imshow(colored_gt_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FCN16:\n\tMissing key(s) in state_dict: \"block1.0.weight\", \"block1.0.bias\", \"block1.2.weight\", \"block1.2.bias\", \"block2.0.weight\", \"block2.0.bias\", \"block2.2.weight\", \"block2.2.bias\", \"block3.0.weight\", \"block3.0.bias\", \"block3.2.weight\", \"block3.2.bias\", \"block3.4.weight\", \"block3.4.bias\", \"block4.0.weight\", \"block4.0.bias\", \"block4.2.weight\", \"block4.2.bias\", \"block4.4.weight\", \"block4.4.bias\", \"block5.0.weight\", \"block5.0.bias\", \"block5.2.weight\", \"block5.2.bias\", \"block5.4.weight\", \"block5.4.bias\", \"fc6.0.weight\", \"fc6.0.bias\", \"fc7.0.weight\", \"fc7.0.bias\", \"block_score.weight\", \"block_score.bias\", \"score_pool4.weight\", \"score_pool4.bias\", \"upscore2.weight\", \"upscore16.weight\". \n\tUnexpected key(s) in state_dict: \"features.0.weight\", \"features.0.bias\", \"features.2.weight\", \"features.2.bias\", \"features.5.weight\", \"features.5.bias\", \"features.7.weight\", \"features.7.bias\", \"features.10.weight\", \"features.10.bias\", \"features.12.weight\", \"features.12.bias\", \"features.14.weight\", \"features.14.bias\", \"features.17.weight\", \"features.17.bias\", \"features.19.weight\", \"features.19.bias\", \"features.21.weight\", \"features.21.bias\", \"features.24.weight\", \"features.24.bias\", \"features.26.weight\", \"features.26.bias\", \"features.28.weight\", \"features.28.bias\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.3.weight\", \"classifier.3.bias\", \"classifier.6.weight\", \"classifier.6.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-4fdc20212cd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdevice_gpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfcn_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFCN16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfcn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_state_dict_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mbase_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D:\\VOCtrainval_25-May-2011\\TrainVal\\VOCdevkit\\VOC2011\\JPEGImages/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0minference_dense_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfcn_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'2007_000042'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1481\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m-> 1483\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m   1484\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FCN16:\n\tMissing key(s) in state_dict: \"block1.0.weight\", \"block1.0.bias\", \"block1.2.weight\", \"block1.2.bias\", \"block2.0.weight\", \"block2.0.bias\", \"block2.2.weight\", \"block2.2.bias\", \"block3.0.weight\", \"block3.0.bias\", \"block3.2.weight\", \"block3.2.bias\", \"block3.4.weight\", \"block3.4.bias\", \"block4.0.weight\", \"block4.0.bias\", \"block4.2.weight\", \"block4.2.bias\", \"block4.4.weight\", \"block4.4.bias\", \"block5.0.weight\", \"block5.0.bias\", \"block5.2.weight\", \"block5.2.bias\", \"block5.4.weight\", \"block5.4.bias\", \"fc6.0.weight\", \"fc6.0.bias\", \"fc7.0.weight\", \"fc7.0.bias\", \"block_score.weight\", \"block_score.bias\", \"score_pool4.weight\", \"score_pool4.bias\", \"upscore2.weight\", \"upscore16.weight\". \n\tUnexpected key(s) in state_dict: \"features.0.weight\", \"features.0.bias\", \"features.2.weight\", \"features.2.bias\", \"features.5.weight\", \"features.5.bias\", \"features.7.weight\", \"features.7.bias\", \"features.10.weight\", \"features.10.bias\", \"features.12.weight\", \"features.12.bias\", \"features.14.weight\", \"features.14.bias\", \"features.17.weight\", \"features.17.bias\", \"features.19.weight\", \"features.19.bias\", \"features.21.weight\", \"features.21.bias\", \"features.24.weight\", \"features.24.bias\", \"features.26.weight\", \"features.26.bias\", \"features.28.weight\", \"features.28.bias\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.3.weight\", \"classifier.3.bias\", \"classifier.6.weight\", \"classifier.6.bias\". "
     ]
    }
   ],
   "source": [
    "model_state_dict_path = 'C:/Users/준영/.cache/torch/hub/checkpoints/vgg16-397923af.pth'\n",
    "model = models.vgg16(pretrained=False)\n",
    "device_gpu = torch.device(\"cuda\")\n",
    "fcn_model = FCN16(model)\n",
    "fcn_model.load_state_dict(torch.load(model_state_dict_path, map_location = device))\n",
    "base_path = 'D:\\VOCtrainval_25-May-2011\\TrainVal\\VOCdevkit\\VOC2011\\JPEGImages/'\n",
    "inference_dense_label(fcn_model, base_path,'2007_000042')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3791ab1f367e35806cd16f76b036a5d685685d81b7c1c68295fd30b1e69059e4"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('py36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
